{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palit-ishan/Traffic-and-Road-Signs-Object-Detection-Project/blob/main/Road_Signs_Detection_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "# Train a custom object detection model for Road Sign Detection with TensorFlow Lite Model Maker\n",
        "\n",
        "We will use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Road Signs and put the TFLite model on a Raspberry Pi.\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Install the required packages\n",
        "We will install the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library we will use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35BJmtVpAP_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31588d9a-af15-4f34-d286-883da956aeec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.3/577.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.8/203.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/128.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "plotnine 0.10.1 requires matplotlib>=3.5.0, but you have matplotlib 3.4.3 which is incompatible.\n",
            "mizani 0.8.1 requires matplotlib>=3.5.0, but you have matplotlib 3.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqNZsAKu4x4Z",
        "outputId": "003fd3aa-ce48-46c5-9cc7-3c85dece776f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.9/dist-packages (1.23.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "7GqEzvlfftZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset - Mounting the Dataset in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jInSvMYm0itV",
        "outputId": "ea2c2249-b833-41ad-ffa5-a2912f233de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Training the object detection model\n",
        "\n",
        "### Loading the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model. Here we use 745 images to train the model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before. Validation is done on 132 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'drive/MyDrive/Project_Test_Data/train_jpg',\n",
        "    'drive/MyDrive/Project_Test_Data/train_jpg',\n",
        "    ['trafficlight','stop','speedlimit','crosswalk']\n",
        ")\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'drive/MyDrive/Project_Test_Data/validate_jpg',\n",
        "    'drive/MyDrive/Project_Test_Data/validate_jpg',\n",
        "    ['trafficlight','stop','speedlimit','crosswalk']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRODW5eTmUX2",
        "outputId": "c2da79b2-6733-45b2-9e98-76a6e680fee3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Selecting a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite2 to train our model, as it is at a good tradeoff between Latency and Average Precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite2')\n",
        "spec.config.autoaugment_policy = 'v0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "###Training the TensorFlow model with the training data.\n",
        "\n",
        "* Setting `epochs = 50`, which means it will go through the training dataset 50 times. We will look at the validation accuracy during training and stop when we see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 16` here so you will see that it takes 46 steps to go through the 745 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MClfpsJAfda",
        "outputId": "e15fbd1c-7b92-48c9-e102-7ac63d001c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "46/46 [==============================] - 106s 1s/step - det_loss: 1.3580 - cls_loss: 0.9094 - box_loss: 0.0090 - reg_l2_loss: 0.0762 - loss: 1.4342 - learning_rate: 0.0140 - gradient_norm: 1.7366 - val_det_loss: 0.5136 - val_cls_loss: 0.3527 - val_box_loss: 0.0032 - val_reg_l2_loss: 0.0763 - val_loss: 0.5899\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 47s 1s/step - det_loss: 0.6388 - cls_loss: 0.4357 - box_loss: 0.0041 - reg_l2_loss: 0.0764 - loss: 0.7152 - learning_rate: 0.0200 - gradient_norm: 2.7643 - val_det_loss: 0.4537 - val_cls_loss: 0.2409 - val_box_loss: 0.0043 - val_reg_l2_loss: 0.0765 - val_loss: 0.5302\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 46s 999ms/step - det_loss: 0.5203 - cls_loss: 0.3436 - box_loss: 0.0035 - reg_l2_loss: 0.0767 - loss: 0.5969 - learning_rate: 0.0199 - gradient_norm: 3.4135 - val_det_loss: 0.3374 - val_cls_loss: 0.2682 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0769 - val_loss: 0.4143\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 44s 959ms/step - det_loss: 0.4563 - cls_loss: 0.3034 - box_loss: 0.0031 - reg_l2_loss: 0.0770 - loss: 0.5333 - learning_rate: 0.0197 - gradient_norm: 2.6634 - val_det_loss: 0.3129 - val_cls_loss: 0.2136 - val_box_loss: 0.0020 - val_reg_l2_loss: 0.0771 - val_loss: 0.3900\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 59s 1s/step - det_loss: 0.4200 - cls_loss: 0.2800 - box_loss: 0.0028 - reg_l2_loss: 0.0772 - loss: 0.4972 - learning_rate: 0.0196 - gradient_norm: 2.6163 - val_det_loss: 0.2024 - val_cls_loss: 0.1503 - val_box_loss: 0.0010 - val_reg_l2_loss: 0.0773 - val_loss: 0.2797\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 47s 1s/step - det_loss: 0.3892 - cls_loss: 0.2615 - box_loss: 0.0026 - reg_l2_loss: 0.0774 - loss: 0.4666 - learning_rate: 0.0194 - gradient_norm: 2.0325 - val_det_loss: 0.2060 - val_cls_loss: 0.1527 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0774 - val_loss: 0.2834\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 43s 946ms/step - det_loss: 0.3830 - cls_loss: 0.2539 - box_loss: 0.0026 - reg_l2_loss: 0.0775 - loss: 0.4605 - learning_rate: 0.0191 - gradient_norm: 2.3353 - val_det_loss: 0.1924 - val_cls_loss: 0.1434 - val_box_loss: 9.7949e-04 - val_reg_l2_loss: 0.0775 - val_loss: 0.2700\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 46s 1s/step - det_loss: 0.3712 - cls_loss: 0.2476 - box_loss: 0.0025 - reg_l2_loss: 0.0776 - loss: 0.4488 - learning_rate: 0.0189 - gradient_norm: 2.2426 - val_det_loss: 0.1869 - val_cls_loss: 0.1376 - val_box_loss: 9.8510e-04 - val_reg_l2_loss: 0.0776 - val_loss: 0.2645\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 49s 1s/step - det_loss: 0.3413 - cls_loss: 0.2326 - box_loss: 0.0022 - reg_l2_loss: 0.0777 - loss: 0.4190 - learning_rate: 0.0186 - gradient_norm: 2.1276 - val_det_loss: 0.1943 - val_cls_loss: 0.1458 - val_box_loss: 9.7091e-04 - val_reg_l2_loss: 0.0777 - val_loss: 0.2720\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 51s 1s/step - det_loss: 0.3288 - cls_loss: 0.2235 - box_loss: 0.0021 - reg_l2_loss: 0.0778 - loss: 0.4066 - learning_rate: 0.0182 - gradient_norm: 1.9149 - val_det_loss: 0.1959 - val_cls_loss: 0.1369 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0778 - val_loss: 0.2737\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 46s 1000ms/step - det_loss: 0.3419 - cls_loss: 0.2372 - box_loss: 0.0021 - reg_l2_loss: 0.0778 - loss: 0.4197 - learning_rate: 0.0178 - gradient_norm: 2.3280 - val_det_loss: 0.1652 - val_cls_loss: 0.1232 - val_box_loss: 8.4083e-04 - val_reg_l2_loss: 0.0779 - val_loss: 0.2431\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 45s 977ms/step - det_loss: 0.2922 - cls_loss: 0.2019 - box_loss: 0.0018 - reg_l2_loss: 0.0779 - loss: 0.3702 - learning_rate: 0.0174 - gradient_norm: 2.0084 - val_det_loss: 0.1674 - val_cls_loss: 0.1201 - val_box_loss: 9.4612e-04 - val_reg_l2_loss: 0.0780 - val_loss: 0.2453\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 47s 1s/step - det_loss: 0.2937 - cls_loss: 0.2034 - box_loss: 0.0018 - reg_l2_loss: 0.0780 - loss: 0.3717 - learning_rate: 0.0170 - gradient_norm: 2.0262 - val_det_loss: 0.1621 - val_cls_loss: 0.1182 - val_box_loss: 8.7801e-04 - val_reg_l2_loss: 0.0781 - val_loss: 0.2401\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 44s 965ms/step - det_loss: 0.2771 - cls_loss: 0.1919 - box_loss: 0.0017 - reg_l2_loss: 0.0781 - loss: 0.3552 - learning_rate: 0.0165 - gradient_norm: 1.7614 - val_det_loss: 0.1584 - val_cls_loss: 0.1272 - val_box_loss: 6.2369e-04 - val_reg_l2_loss: 0.0781 - val_loss: 0.2365\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 52s 1s/step - det_loss: 0.2809 - cls_loss: 0.1925 - box_loss: 0.0018 - reg_l2_loss: 0.0781 - loss: 0.3590 - learning_rate: 0.0160 - gradient_norm: 1.9019 - val_det_loss: 0.1389 - val_cls_loss: 0.1057 - val_box_loss: 6.6467e-04 - val_reg_l2_loss: 0.0781 - val_loss: 0.2171\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 44s 960ms/step - det_loss: 0.2705 - cls_loss: 0.1904 - box_loss: 0.0016 - reg_l2_loss: 0.0781 - loss: 0.3487 - learning_rate: 0.0155 - gradient_norm: 1.8157 - val_det_loss: 0.1289 - val_cls_loss: 0.1003 - val_box_loss: 5.7195e-04 - val_reg_l2_loss: 0.0782 - val_loss: 0.2071\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 46s 994ms/step - det_loss: 0.2821 - cls_loss: 0.1934 - box_loss: 0.0018 - reg_l2_loss: 0.0782 - loss: 0.3603 - learning_rate: 0.0149 - gradient_norm: 2.0836 - val_det_loss: 0.1462 - val_cls_loss: 0.1117 - val_box_loss: 6.9046e-04 - val_reg_l2_loss: 0.0782 - val_loss: 0.2245\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 46s 1s/step - det_loss: 0.2758 - cls_loss: 0.1864 - box_loss: 0.0018 - reg_l2_loss: 0.0782 - loss: 0.3541 - learning_rate: 0.0143 - gradient_norm: 1.9095 - val_det_loss: 0.1328 - val_cls_loss: 0.1043 - val_box_loss: 5.6948e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.2110\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 44s 967ms/step - det_loss: 0.2648 - cls_loss: 0.1820 - box_loss: 0.0017 - reg_l2_loss: 0.0783 - loss: 0.3430 - learning_rate: 0.0138 - gradient_norm: 1.8083 - val_det_loss: 0.1347 - val_cls_loss: 0.1031 - val_box_loss: 6.3237e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.2130\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 52s 1s/step - det_loss: 0.2576 - cls_loss: 0.1817 - box_loss: 0.0015 - reg_l2_loss: 0.0783 - loss: 0.3359 - learning_rate: 0.0132 - gradient_norm: 1.8334 - val_det_loss: 0.1321 - val_cls_loss: 0.1030 - val_box_loss: 5.8094e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.2104\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 44s 970ms/step - det_loss: 0.2378 - cls_loss: 0.1674 - box_loss: 0.0014 - reg_l2_loss: 0.0783 - loss: 0.3161 - learning_rate: 0.0125 - gradient_norm: 1.7875 - val_det_loss: 0.1179 - val_cls_loss: 0.0970 - val_box_loss: 4.1840e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1962\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 45s 987ms/step - det_loss: 0.2350 - cls_loss: 0.1655 - box_loss: 0.0014 - reg_l2_loss: 0.0783 - loss: 0.3133 - learning_rate: 0.0119 - gradient_norm: 1.7538 - val_det_loss: 0.1192 - val_cls_loss: 0.0957 - val_box_loss: 4.7096e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1976\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 48s 1s/step - det_loss: 0.2393 - cls_loss: 0.1642 - box_loss: 0.0015 - reg_l2_loss: 0.0783 - loss: 0.3176 - learning_rate: 0.0113 - gradient_norm: 1.7117 - val_det_loss: 0.1205 - val_cls_loss: 0.0914 - val_box_loss: 5.8243e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1988\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 44s 961ms/step - det_loss: 0.2217 - cls_loss: 0.1564 - box_loss: 0.0013 - reg_l2_loss: 0.0783 - loss: 0.3000 - learning_rate: 0.0106 - gradient_norm: 1.6432 - val_det_loss: 0.1063 - val_cls_loss: 0.0872 - val_box_loss: 3.8250e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1847\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 52s 1s/step - det_loss: 0.2366 - cls_loss: 0.1660 - box_loss: 0.0014 - reg_l2_loss: 0.0783 - loss: 0.3149 - learning_rate: 0.0100 - gradient_norm: 1.6217 - val_det_loss: 0.1121 - val_cls_loss: 0.0896 - val_box_loss: 4.4888e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1904\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 45s 977ms/step - det_loss: 0.2118 - cls_loss: 0.1518 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2901 - learning_rate: 0.0094 - gradient_norm: 1.6566 - val_det_loss: 0.1076 - val_cls_loss: 0.0892 - val_box_loss: 3.6854e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1860\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 45s 979ms/step - det_loss: 0.2188 - cls_loss: 0.1566 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2971 - learning_rate: 0.0087 - gradient_norm: 1.5819 - val_det_loss: 0.1183 - val_cls_loss: 0.0982 - val_box_loss: 4.0289e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1967\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 47s 1s/step - det_loss: 0.2118 - cls_loss: 0.1523 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2902 - learning_rate: 0.0081 - gradient_norm: 1.6014 - val_det_loss: 0.1104 - val_cls_loss: 0.0905 - val_box_loss: 3.9788e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1888\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 44s 954ms/step - det_loss: 0.2117 - cls_loss: 0.1523 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2900 - learning_rate: 0.0075 - gradient_norm: 1.7441 - val_det_loss: 0.1061 - val_cls_loss: 0.0900 - val_box_loss: 3.2194e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1845\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 53s 1s/step - det_loss: 0.2107 - cls_loss: 0.1512 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2891 - learning_rate: 0.0068 - gradient_norm: 1.6361 - val_det_loss: 0.1128 - val_cls_loss: 0.0908 - val_box_loss: 4.3974e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1911\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 44s 953ms/step - det_loss: 0.2131 - cls_loss: 0.1505 - box_loss: 0.0013 - reg_l2_loss: 0.0783 - loss: 0.2914 - learning_rate: 0.0062 - gradient_norm: 1.5829 - val_det_loss: 0.1017 - val_cls_loss: 0.0844 - val_box_loss: 3.4467e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1800\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 46s 994ms/step - det_loss: 0.2174 - cls_loss: 0.1566 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2957 - learning_rate: 0.0057 - gradient_norm: 1.6645 - val_det_loss: 0.0996 - val_cls_loss: 0.0830 - val_box_loss: 3.3168e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1779\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 49s 1s/step - det_loss: 0.2039 - cls_loss: 0.1455 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2822 - learning_rate: 0.0051 - gradient_norm: 1.4438 - val_det_loss: 0.0997 - val_cls_loss: 0.0811 - val_box_loss: 3.7286e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1780\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 44s 962ms/step - det_loss: 0.2044 - cls_loss: 0.1458 - box_loss: 0.0012 - reg_l2_loss: 0.0783 - loss: 0.2827 - learning_rate: 0.0045 - gradient_norm: 1.4155 - val_det_loss: 0.0995 - val_cls_loss: 0.0832 - val_box_loss: 3.2516e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1778\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 53s 1s/step - det_loss: 0.1877 - cls_loss: 0.1357 - box_loss: 0.0010 - reg_l2_loss: 0.0783 - loss: 0.2660 - learning_rate: 0.0040 - gradient_norm: 1.3819 - val_det_loss: 0.0980 - val_cls_loss: 0.0824 - val_box_loss: 3.1157e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1763\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 45s 984ms/step - det_loss: 0.1940 - cls_loss: 0.1404 - box_loss: 0.0011 - reg_l2_loss: 0.0783 - loss: 0.2723 - learning_rate: 0.0035 - gradient_norm: 1.5843 - val_det_loss: 0.0957 - val_cls_loss: 0.0806 - val_box_loss: 3.0202e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1740\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 49s 1s/step - det_loss: 0.1799 - cls_loss: 0.1311 - box_loss: 9.7471e-04 - reg_l2_loss: 0.0783 - loss: 0.2582 - learning_rate: 0.0030 - gradient_norm: 1.4135 - val_det_loss: 0.0938 - val_cls_loss: 0.0786 - val_box_loss: 3.0350e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1721\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 45s 973ms/step - det_loss: 0.1872 - cls_loss: 0.1348 - box_loss: 0.0010 - reg_l2_loss: 0.0783 - loss: 0.2654 - learning_rate: 0.0026 - gradient_norm: 1.4089 - val_det_loss: 0.0963 - val_cls_loss: 0.0813 - val_box_loss: 3.0121e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1746\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 46s 998ms/step - det_loss: 0.1924 - cls_loss: 0.1405 - box_loss: 0.0010 - reg_l2_loss: 0.0783 - loss: 0.2707 - learning_rate: 0.0022 - gradient_norm: 1.3379 - val_det_loss: 0.0935 - val_cls_loss: 0.0790 - val_box_loss: 2.8978e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1717\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 51s 1s/step - det_loss: 0.1782 - cls_loss: 0.1305 - box_loss: 9.5406e-04 - reg_l2_loss: 0.0783 - loss: 0.2565 - learning_rate: 0.0018 - gradient_norm: 1.3576 - val_det_loss: 0.0938 - val_cls_loss: 0.0790 - val_box_loss: 2.9471e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1720\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 46s 1s/step - det_loss: 0.1755 - cls_loss: 0.1289 - box_loss: 9.3069e-04 - reg_l2_loss: 0.0783 - loss: 0.2537 - learning_rate: 0.0015 - gradient_norm: 1.3729 - val_det_loss: 0.0940 - val_cls_loss: 0.0794 - val_box_loss: 2.9259e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1723\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 46s 1s/step - det_loss: 0.1981 - cls_loss: 0.1432 - box_loss: 0.0011 - reg_l2_loss: 0.0783 - loss: 0.2763 - learning_rate: 0.0011 - gradient_norm: 1.4824 - val_det_loss: 0.0933 - val_cls_loss: 0.0788 - val_box_loss: 2.8978e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1716\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 44s 971ms/step - det_loss: 0.1793 - cls_loss: 0.1304 - box_loss: 9.7934e-04 - reg_l2_loss: 0.0783 - loss: 0.2576 - learning_rate: 8.5762e-04 - gradient_norm: 1.4105 - val_det_loss: 0.0926 - val_cls_loss: 0.0782 - val_box_loss: 2.8759e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1709\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 46s 1s/step - det_loss: 0.2007 - cls_loss: 0.1448 - box_loss: 0.0011 - reg_l2_loss: 0.0783 - loss: 0.2789 - learning_rate: 6.1709e-04 - gradient_norm: 1.6075 - val_det_loss: 0.0915 - val_cls_loss: 0.0778 - val_box_loss: 2.7376e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1697\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 50s 1s/step - det_loss: 0.1865 - cls_loss: 0.1355 - box_loss: 0.0010 - reg_l2_loss: 0.0783 - loss: 0.2648 - learning_rate: 4.1511e-04 - gradient_norm: 1.3925 - val_det_loss: 0.0922 - val_cls_loss: 0.0780 - val_box_loss: 2.8411e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1705\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 46s 994ms/step - det_loss: 0.1834 - cls_loss: 0.1340 - box_loss: 9.8824e-04 - reg_l2_loss: 0.0783 - loss: 0.2617 - learning_rate: 2.5252e-04 - gradient_norm: 1.3914 - val_det_loss: 0.0911 - val_cls_loss: 0.0772 - val_box_loss: 2.7753e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1694\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 47s 1s/step - det_loss: 0.1811 - cls_loss: 0.1308 - box_loss: 0.0010 - reg_l2_loss: 0.0783 - loss: 0.2594 - learning_rate: 1.2998e-04 - gradient_norm: 1.4879 - val_det_loss: 0.0914 - val_cls_loss: 0.0777 - val_box_loss: 2.7501e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1697\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 44s 965ms/step - det_loss: 0.1755 - cls_loss: 0.1285 - box_loss: 9.3995e-04 - reg_l2_loss: 0.0783 - loss: 0.2538 - learning_rate: 4.8007e-05 - gradient_norm: 1.2874 - val_det_loss: 0.0914 - val_cls_loss: 0.0776 - val_box_loss: 2.7513e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1697\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 48s 1s/step - det_loss: 0.1828 - cls_loss: 0.1337 - box_loss: 9.8284e-04 - reg_l2_loss: 0.0783 - loss: 0.2611 - learning_rate: 6.9242e-06 - gradient_norm: 1.4138 - val_det_loss: 0.0914 - val_cls_loss: 0.0776 - val_box_loss: 2.7590e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1696\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 52s 1s/step - det_loss: 0.1803 - cls_loss: 0.1316 - box_loss: 9.7333e-04 - reg_l2_loss: 0.0783 - loss: 0.2586 - learning_rate: 6.9053e-06 - gradient_norm: 1.3014 - val_det_loss: 0.0914 - val_cls_loss: 0.0776 - val_box_loss: 2.7621e-04 - val_reg_l2_loss: 0.0783 - val_loss: 0.1697\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec,batch_size = 16,train_whole_model=True, epochs=50, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Evaluating the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 132 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 2 step to go through the 132 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TZPxPs7RNMu",
        "outputId": "f0fda946-ed95-47fe-915a-b81e84a2b733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUqEpcYwAg8L",
        "outputId": "d528d16b-95c2-45eb-f6a5-d7dc382ca7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 24s 3s/step\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.64514923,\n",
              " 'AP50': 0.78636855,\n",
              " 'AP75': 0.7598101,\n",
              " 'APs': 0.48135948,\n",
              " 'APm': 0.8445239,\n",
              " 'APl': 0.9590602,\n",
              " 'ARmax1': 0.58477634,\n",
              " 'ARmax10': 0.7166675,\n",
              " 'ARmax100': 0.7193265,\n",
              " 'ARs': 0.57237744,\n",
              " 'ARm': 0.8945395,\n",
              " 'ARl': 0.96666664,\n",
              " 'AP_/trafficlight': 0.45070502,\n",
              " 'AP_/stop': 0.66664034,\n",
              " 'AP_/speedlimit': 0.86242515,\n",
              " 'AP_/crosswalk': 0.6008265}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='road_signs_efficientdet_lite2_aug_v0_b16_ep50.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbl8z9_wBPlr",
        "outputId": "e6c4fd0c-39a7-4423-edd3-a51e7bf00710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "132/132 [==============================] - 1332s 10s/step\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.62906927,\n",
              " 'AP50': 0.78477293,\n",
              " 'AP75': 0.7391676,\n",
              " 'APs': 0.46777886,\n",
              " 'APm': 0.81886834,\n",
              " 'APl': 0.9587459,\n",
              " 'ARmax1': 0.56349117,\n",
              " 'ARmax10': 0.67622435,\n",
              " 'ARmax100': 0.67622435,\n",
              " 'ARs': 0.52254903,\n",
              " 'ARm': 0.8516886,\n",
              " 'ARl': 0.96,\n",
              " 'AP_/trafficlight': 0.44190758,\n",
              " 'AP_/stop': 0.6226553,\n",
              " 'AP_/speedlimit': 0.8511243,\n",
              " 'AP_/crosswalk': 0.60059}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate_tflite('road_signs_efficientdet_lite2_aug_v0_b16_ep50.tflite', val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing Model on a still image to see output with Bounding Box"
      ],
      "metadata": {
        "id": "QKWuv1mooMnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['trafficlight','stop','speedlimit','crosswalk']"
      ],
      "metadata": {
        "id": "VCyFVq_I19Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to use model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"road_signs_efficientdet_lite2_aug_v0_b16_ep50.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "signature_fn = interpreter.get_signature_runner()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n"
      ],
      "metadata": {
        "id": "McukYs1Ktc7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPy57avT2R59",
        "outputId": "870917b9-9af6-46f4-ff90-ade890459a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'serving_default_images:0',\n",
              "  'index': 0,\n",
              "  'shape': array([  1, 448, 448,   3], dtype=int32),\n",
              "  'shape_signature': array([  1, 448, 448,   3], dtype=int32),\n",
              "  'dtype': numpy.uint8,\n",
              "  'quantization': (0.0078125, 127),\n",
              "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
              "   'zero_points': array([127], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Getting image dimension '''\n",
        "rdim = (input_details[0]['shape'][1], input_details[0]['shape'][2])"
      ],
      "metadata": {
        "id": "CyBtiUt_JmFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Providing Path for Image\n",
        "image_path = 'stop_cross_walk.jpg'"
      ],
      "metadata": {
        "id": "qbmFMdJmuQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data_type = input_details[0][\"dtype\"]\n",
        "or_image = cv2.imread(image_path)\n",
        "image = cv2.resize(or_image,rdim)\n",
        "image = np.array(image, dtype=input_data_type)\n",
        "image = np.expand_dims(image, axis = 0)"
      ],
      "metadata": {
        "id": "L3yZC7rkuLX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "or_image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaTjOjGB27N3",
        "outputId": "f9a76cc8-bed9-4833-8688-c7ad5e00a80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(607, 940, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = signature_fn(images=image)\n",
        "count = int(np.squeeze(output['output_0']))\n",
        "scores = np.squeeze(output['output_1'])\n",
        "classes = np.squeeze(output['output_2'])\n",
        "boxes = np.squeeze(output['output_3'])"
      ],
      "metadata": {
        "id": "p6puU4eCugK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(count):\n",
        "  if scores[i] >= 0.3:        # Setting the confidence level above 0.3 to show bounding boxes\n",
        "    result = {\n",
        "      'bounding_box': boxes[i],\n",
        "      'class_id': classes[i],\n",
        "      'score': scores[i]\n",
        "    }\n",
        "    results.append(result)"
      ],
      "metadata": {
        "id": "YN38t8VrugSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLORS = np.random.randint(0, 254, size=(len(classes), 3), dtype=np.uint8)\n",
        "for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute\n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * or_image.shape[1])\n",
        "    xmax = int(xmax * or_image.shape[1])\n",
        "    ymin = int(ymin * or_image.shape[0])\n",
        "    ymax = int(ymax * or_image.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(or_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(labels[class_id], obj['score'] * 100)\n",
        "    cv2.putText(or_image, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    \n",
        "original_uint8 = or_image.astype(np.uint8)"
      ],
      "metadata": {
        "id": "qnXqhA8l8rPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('output.png', original_uint8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAPG1UQ62ACC",
        "outputId": "edca22a5-32fc-44d2-a282-a5fc40781bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}